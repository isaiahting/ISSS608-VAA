{
  "hash": "75168cef35a3675908b8e3e5dac074ae",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"InHandsEx5\"\nauthor: \"Joshua TING\"\ndate: \"May 11, 2024\"\ndate-modified: \"last-modified\"\nexecute:\n  eval: true\n  echo: true\n  warning: false\n  freeze: true\n---\n\n\n## Getting Started\n\n### Installing & Launching R packages\n\nThe following R packages for handling, processing, wrangling, analysing and visualising text data will be used:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(tidyverse, readtext, quanteda, tidytext)\n```\n:::\n\n\nPackages:\n\n[readtext](https://readtext.quanteda.io/) is a one-function package that does exactly what it says on the tin: It reads files containing text, along with any associated document-level metadata, which we call “docvars”, for document variables. Plain text files do not have docvars, but other forms such as .csv, .tab, .xml, and .json files usually do.\n\n[**quanteda**](https://quanteda.io/) is a family of package is designed for R users needing to apply natural language processing to texts, from documents to final analysis. These are the following packages:\n\n-   **quanteda**: contains all of the core natural language processing and textual data management functions\n\n-   **quanteda.textmodels**: contains all of the text models and supporting functions, namely the `textmodel_*()` functions. This was split from the main package with the v2 release\n\n-   **quanteda.textstats**: statistics for textual data, namely the `textstat_*()` functions, split with the v3 release\n\n-   **quanteda.textplots**: plots for textual data, namely the `textplot_*()` functions, split with the v3 release\n\nCombing all files into one file\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data <- readtext(paste0(\"data/MC1/articles/*\"))\n```\n:::\n\n\n::: callout-note\n## \\* Wildcard Function\n\n\\* is wildcard that enables to read all files in the \"article\" folder\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_text <- corpus(text_data)\nsummary(corpus_text, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCorpus consisting of 338 documents, showing 5 documents:\n\n                                   Text Types Tokens Sentences\n Alvarez PLC__0__0__Haacklee Herald.txt   206    433        18\n    Alvarez PLC__0__0__Lomark Daily.txt   102    170        12\n   Alvarez PLC__0__0__The News Buoy.txt    90    200         9\n Alvarez PLC__0__1__Haacklee Herald.txt    96    187         8\n    Alvarez PLC__0__1__Lomark Daily.txt   241    504        21\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nusenet_words <- text_data %>%\n  unnest_tokens(word, text) %>%\n  filter(str_detect(word, \"[a-z']$\"),\n  !word %in% stop_words$word)\n```\n:::\n\n\n::: callout-note\n## Stop Words\n\n[**Stop words**](https://en.wikipedia.org/wiki/Stop_word) are the words in a **stop list** (or ***stoplist*** or *negative dictionary*) which are filtered out (i.e. stopped) before or after processing of natural language data (text) because they are deemed insignificant.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nusenet_words %>%\n  count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nreadtext object consisting of 3261 documents and 0 docvars.\n# A data frame: 3,261 × 3\n  word             n text     \n  <chr>        <int> <chr>    \n1 fishing       2177 \"\\\"\\\"...\"\n2 sustainable   1525 \"\\\"\\\"...\"\n3 company       1036 \"\\\"\\\"...\"\n4 practices      838 \"\\\"\\\"...\"\n5 industry       715 \"\\\"\\\"...\"\n6 transactions   696 \"\\\"\\\"...\"\n# ℹ 3,255 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data_splitted <- text_data %>%\n  separate_wider_delim(\"doc_id\",\n                        delim = \"__0__\",\n                        names = c(\"X\", \"Y\"),\n                        too_few = \"align_end\")\n```\n:::\n\n\n`separate_wider_delim()` function from the `tidyr` package (part of the `tidyverse`). It separates a single column into multiple columns based on a specified delimiter. In this case, it's separating the \"doc_id\" column.\n\n`delim = \"__0__\",`: This argument specifies the delimiter used to separate the values in the \"doc_id\" column. Here, the delimiter is \"**0**\".\n\n`names = c(\"X\", \"Y\"),`: This argument specifies the names for the new columns created after splitting the \"doc_id\" column. In this case, the new columns will be named \"X\" and \"Y\".\n\n`too_few = \"align_end\")`: This argument specifies how the function should handle cases where there are fewer pieces than the number of new columns specified in `names`. Here, `\"align_end\"` means that any missing pieces will be treated as empty strings (`\"\"`) and added to the end of the new columns.\n\n::: callout-note\n## String r\n\n[String r](https://stringr.tidyverse.org/) can help to split words\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(jsonlite, tidyverse)\n```\n:::\n\n\nIn the code chunk below, `from JSON()` of **jsonlite** package is used to import *mc1.json* into R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmc1_data <- fromJSON(\"data/MC1/mc1.json\")\n```\n:::\n\n\nThis data contains of link and nodes.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}